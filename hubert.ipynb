{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sahilk45/PyTorch-1Go/blob/main/hubert.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "de8b9b51",
      "metadata": {
        "id": "de8b9b51",
        "outputId": "09defa20-8f51-4882-e984-3db83a4076b3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/staru/anaconda3/envs/wave2vec/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import os\n",
        "import sys\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import librosa\n",
        "import warnings\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "from collections import defaultdict\n",
        "import json\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch.optim import AdamW\n",
        "from torch.optim.lr_scheduler import LinearLR\n",
        "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, precision_recall_fscore_support\n",
        "from transformers import Wav2Vec2FeatureExtractor, HubertForSequenceClassification\n",
        "warnings.filterwarnings('ignore')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7973fc22",
      "metadata": {
        "id": "7973fc22",
        "outputId": "d6277518-62e6-41f8-eaf5-414a03f568cc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Configuration loaded\n",
            "  Device: cuda\n",
            "  Emotions: ['Anger', 'Disgust', 'Fear', 'Happy', 'Sad', 'Neutral']\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# ============================================================================\n",
        "# 1. CONFIGURATION\n",
        "# ============================================================================\n",
        "\n",
        "def set_seed(seed=42):\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "\n",
        "SEED = 42\n",
        "set_seed(SEED)\n",
        "\n",
        "CONFIG = {\n",
        "    'random_seed': SEED,\n",
        "    'dataset_path': './aug_pitch_audio',\n",
        "    'sr': 16000,\n",
        "    'max_duration': 10.0,\n",
        "    'train_split': 0.8,\n",
        "    'val_split': 0.1,\n",
        "    'test_split': 0.1,\n",
        "    'batch_size': 8,\n",
        "    'epochs': 20,\n",
        "    'learning_rate': 2e-5,\n",
        "    'max_length': 160000,\n",
        "    'hubert_model': 'facebook/hubert-base-superb',\n",
        "    'device': torch.device('cuda' if torch.cuda.is_available() else 'cpu'),\n",
        "}\n",
        "\n",
        "EMOTIONS = ['Anger', 'Disgust', 'Fear', 'Happy', 'Sad', 'Neutral']\n",
        "EMOTION_TO_IDX = {emotion: idx for idx, emotion in enumerate(EMOTIONS)}\n",
        "IDX_TO_EMOTION = {idx: emotion for emotion, idx in EMOTION_TO_IDX.items()}\n",
        "\n",
        "print('✓ Configuration loaded')\n",
        "print(f'  Device: {CONFIG[\"device\"]}')\n",
        "print(f'  Emotions: {EMOTIONS}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "12e3d8fa",
      "metadata": {
        "id": "12e3d8fa",
        "outputId": "be930606-3811-4241-8281-78df232dd40a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading dataset...\n",
            "\n",
            "✓ Dataset loaded successfully\n",
            "Samples per emotion:\n",
            "  Anger: 1350\n",
            "  Disgust: 1350\n",
            "  Fear: 1350\n",
            "  Happy: 1350\n",
            "  Sad: 1350\n",
            "  Neutral: 1350\n",
            "Total samples: 8100\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# ============================================================================\n",
        "# 2. DATA LOADING\n",
        "# ============================================================================\n",
        "\n",
        "def load_audio_data(dataset_path, emotions=EMOTIONS, sr=16000, max_duration=10.0):\n",
        "    \"\"\"Load audio files from emotion folders.\"\"\"\n",
        "    file_paths = []\n",
        "    stats = defaultdict(int)\n",
        "    errors = []\n",
        "\n",
        "    for emotion in emotions:\n",
        "        emotion_dir = os.path.join(dataset_path, emotion)\n",
        "\n",
        "        if not os.path.exists(emotion_dir):\n",
        "            print(f'⚠ Warning: {emotion_dir} not found')\n",
        "            continue\n",
        "\n",
        "        audio_files = []\n",
        "        for ext in ['*.mp3', '*.wav', '*.ogg', '*.flac']:\n",
        "            audio_files.extend(Path(emotion_dir).glob(ext))\n",
        "\n",
        "        for audio_path in audio_files:\n",
        "            try:\n",
        "                y, _ = librosa.load(str(audio_path), sr=sr, duration=max_duration)\n",
        "                if len(y) > 0:\n",
        "                    file_paths.append((str(audio_path), emotion))\n",
        "                    stats[emotion] += 1\n",
        "            except Exception as e:\n",
        "                errors.append((str(audio_path), str(e)))\n",
        "\n",
        "    print('\\n✓ Dataset loaded successfully')\n",
        "    print('Samples per emotion:')\n",
        "    for emotion in emotions:\n",
        "        count = stats.get(emotion, 0)\n",
        "        print(f'  {emotion}: {count}')\n",
        "    print(f'Total samples: {sum(stats.values())}')\n",
        "\n",
        "    if errors:\n",
        "        print(f'⚠ {len(errors)} files had errors')\n",
        "\n",
        "    return file_paths, dict(stats)\n",
        "\n",
        "print('Loading dataset...')\n",
        "file_paths, stats = load_audio_data(CONFIG['dataset_path'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0fffc9e3",
      "metadata": {
        "id": "0fffc9e3",
        "outputId": "f8cdd1cd-16a9-439e-93f9-3787d1f729c8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Data split completed\n",
            "  Train: 6480, Val: 810, Test: 810\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# ============================================================================\n",
        "# 3. STRATIFIED SPLIT\n",
        "# ============================================================================\n",
        "\n",
        "def stratified_split(file_paths, train_ratio=0.8, val_ratio=0.1, test_ratio=0.1, seed=42):\n",
        "    \"\"\"Stratified split maintaining emotion distribution.\"\"\"\n",
        "    emotion_groups = defaultdict(list)\n",
        "    for path, emotion in file_paths:\n",
        "        emotion_groups[emotion].append((path, emotion))\n",
        "\n",
        "    train_set, val_set, test_set = [], [], []\n",
        "    np.random.seed(seed)\n",
        "\n",
        "    for emotion, samples in emotion_groups.items():\n",
        "        np.random.shuffle(samples)\n",
        "        n_train = int(len(samples) * train_ratio)\n",
        "        n_val = int(len(samples) * val_ratio)\n",
        "\n",
        "        train_set.extend(samples[:n_train])\n",
        "        val_set.extend(samples[n_train:n_train+n_val])\n",
        "        test_set.extend(samples[n_train+n_val:])\n",
        "\n",
        "    return train_set, val_set, test_set\n",
        "\n",
        "train_set, val_set, test_set = stratified_split(file_paths, seed=SEED)\n",
        "\n",
        "print('✓ Data split completed')\n",
        "print(f'  Train: {len(train_set)}, Val: {len(val_set)}, Test: {len(test_set)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d0ef7839",
      "metadata": {
        "id": "d0ef7839"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ============================================================================\n",
        "# 4. DATASET CLASS\n",
        "# ============================================================================\n",
        "\n",
        "class EmotionAudioDataset(Dataset):\n",
        "    def __init__(self, file_paths, feature_extractor, sr=16000, max_length=160000):\n",
        "        self.file_paths = file_paths\n",
        "        self.feature_extractor = feature_extractor\n",
        "        self.sr = sr\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.file_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        audio_path, emotion = self.file_paths[idx]\n",
        "\n",
        "        try:\n",
        "            y, sr = librosa.load(audio_path, sr=self.sr)\n",
        "\n",
        "            if len(y) < self.max_length:\n",
        "                y = np.pad(y, (0, self.max_length - len(y)), mode='constant')\n",
        "            else:\n",
        "                y = y[:self.max_length]\n",
        "\n",
        "            inputs = self.feature_extractor(\n",
        "                y, sampling_rate=self.sr, return_tensors='pt', padding=True\n",
        "            )\n",
        "\n",
        "            return {\n",
        "                'input_values': inputs['input_values'].squeeze(),\n",
        "                'labels': torch.tensor(EMOTION_TO_IDX[emotion], dtype=torch.long),\n",
        "                'audio_path': audio_path,\n",
        "                'emotion': emotion\n",
        "            }\n",
        "        except Exception as e:\n",
        "            print(f'Error loading {audio_path}: {e}')\n",
        "            return None\n",
        "\n",
        "def collate_fn(batch):\n",
        "    batch = [item for item in batch if item is not None]\n",
        "    if len(batch) == 0:\n",
        "        return None\n",
        "\n",
        "    input_values = torch.stack([item['input_values'] for item in batch])\n",
        "    labels = torch.stack([item['labels'] for item in batch])\n",
        "\n",
        "    return {\n",
        "        'input_values': input_values,\n",
        "        'labels': labels\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ff9c085b",
      "metadata": {
        "id": "ff9c085b",
        "outputId": "f465ac24-7b9f-490d-d9ae-60a56c777891"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Loading HuBERT model: facebook/hubert-base-superb\n"
          ]
        },
        {
          "ename": "OSError",
          "evalue": "facebook/hubert-base-superb is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
            "File \u001b[0;32m~/anaconda3/envs/wave2vec/lib/python3.10/site-packages/huggingface_hub/utils/_http.py:407\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    406\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 407\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    408\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
            "File \u001b[0;32m~/anaconda3/envs/wave2vec/lib/python3.10/site-packages/requests/models.py:1026\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[0;32m-> 1026\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
            "\u001b[0;31mHTTPError\u001b[0m: 404 Client Error: Not Found for url: https://huggingface.co/facebook/hubert-base-superb/resolve/main/preprocessor_config.json",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mRepositoryNotFoundError\u001b[0m                   Traceback (most recent call last)",
            "File \u001b[0;32m~/anaconda3/envs/wave2vec/lib/python3.10/site-packages/transformers/utils/hub.py:479\u001b[0m, in \u001b[0;36mcached_files\u001b[0;34m(path_or_repo_id, filenames, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(full_filenames) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;66;03m# This is slightly better for only 1 file\u001b[39;00m\n\u001b[0;32m--> 479\u001b[0m     \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    480\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    481\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilenames\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    482\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    483\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    484\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    485\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    486\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    487\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    491\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    492\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
            "File \u001b[0;32m~/anaconda3/envs/wave2vec/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/anaconda3/envs/wave2vec/lib/python3.10/site-packages/huggingface_hub/file_download.py:1010\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, resume_download, force_filename, local_dir_use_symlinks)\u001b[0m\n\u001b[1;32m   1009\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1010\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_hf_hub_download_to_cache_dir\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1011\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Destination\u001b[39;49;00m\n\u001b[1;32m   1012\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1013\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# File info\u001b[39;49;00m\n\u001b[1;32m   1014\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1015\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1016\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1017\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1018\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# HTTP info\u001b[39;49;00m\n\u001b[1;32m   1019\u001b[0m \u001b[43m        \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1020\u001b[0m \u001b[43m        \u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1021\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1022\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1023\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1024\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Additional options\u001b[39;49;00m\n\u001b[1;32m   1025\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1026\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1027\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/anaconda3/envs/wave2vec/lib/python3.10/site-packages/huggingface_hub/file_download.py:1117\u001b[0m, in \u001b[0;36m_hf_hub_download_to_cache_dir\u001b[0;34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001b[0m\n\u001b[1;32m   1116\u001b[0m     \u001b[38;5;66;03m# Otherwise, raise appropriate error\u001b[39;00m\n\u001b[0;32m-> 1117\u001b[0m     \u001b[43m_raise_on_head_call_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhead_call_error\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1119\u001b[0m \u001b[38;5;66;03m# From now on, etag, commit_hash, url and size are not None.\u001b[39;00m\n",
            "File \u001b[0;32m~/anaconda3/envs/wave2vec/lib/python3.10/site-packages/huggingface_hub/file_download.py:1658\u001b[0m, in \u001b[0;36m_raise_on_head_call_error\u001b[0;34m(head_call_error, force_download, local_files_only)\u001b[0m\n\u001b[1;32m   1653\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(head_call_error, (RepositoryNotFoundError, GatedRepoError)) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   1654\u001b[0m     \u001b[38;5;28misinstance\u001b[39m(head_call_error, HfHubHTTPError) \u001b[38;5;129;01mand\u001b[39;00m head_call_error\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m401\u001b[39m\n\u001b[1;32m   1655\u001b[0m ):\n\u001b[1;32m   1656\u001b[0m     \u001b[38;5;66;03m# Repo not found or gated => let's raise the actual error\u001b[39;00m\n\u001b[1;32m   1657\u001b[0m     \u001b[38;5;66;03m# Unauthorized => likely a token issue => let's raise the actual error\u001b[39;00m\n\u001b[0;32m-> 1658\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m head_call_error\n\u001b[1;32m   1659\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1660\u001b[0m     \u001b[38;5;66;03m# Otherwise: most likely a connection issue or Hub downtime => let's warn the user\u001b[39;00m\n",
            "File \u001b[0;32m~/anaconda3/envs/wave2vec/lib/python3.10/site-packages/huggingface_hub/file_download.py:1546\u001b[0m, in \u001b[0;36m_get_metadata_or_catch_error\u001b[0;34m(repo_id, filename, repo_type, revision, endpoint, proxies, etag_timeout, headers, token, local_files_only, relative_filename, storage_folder)\u001b[0m\n\u001b[1;32m   1545\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1546\u001b[0m     metadata \u001b[38;5;241m=\u001b[39m \u001b[43mget_hf_file_metadata\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1547\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mendpoint\u001b[49m\n\u001b[1;32m   1548\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1549\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m EntryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m http_error:\n",
            "File \u001b[0;32m~/anaconda3/envs/wave2vec/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/anaconda3/envs/wave2vec/lib/python3.10/site-packages/huggingface_hub/file_download.py:1463\u001b[0m, in \u001b[0;36mget_hf_file_metadata\u001b[0;34m(url, token, proxies, timeout, library_name, library_version, user_agent, headers, endpoint)\u001b[0m\n\u001b[1;32m   1462\u001b[0m \u001b[38;5;66;03m# Retrieve metadata\u001b[39;00m\n\u001b[0;32m-> 1463\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1464\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHEAD\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1465\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1466\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1467\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1468\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1469\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1470\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1471\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1472\u001b[0m hf_raise_for_status(r)\n",
            "File \u001b[0;32m~/anaconda3/envs/wave2vec/lib/python3.10/site-packages/huggingface_hub/file_download.py:286\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m follow_relative_redirects:\n\u001b[0;32m--> 286\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    287\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    288\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    289\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    293\u001b[0m     \u001b[38;5;66;03m# If redirection, we redirect only relative paths.\u001b[39;00m\n\u001b[1;32m    294\u001b[0m     \u001b[38;5;66;03m# This is useful in case of a renamed repository.\u001b[39;00m\n",
            "File \u001b[0;32m~/anaconda3/envs/wave2vec/lib/python3.10/site-packages/huggingface_hub/file_download.py:310\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    309\u001b[0m response \u001b[38;5;241m=\u001b[39m http_backoff(method\u001b[38;5;241m=\u001b[39mmethod, url\u001b[38;5;241m=\u001b[39murl, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[0;32m--> 310\u001b[0m \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
            "File \u001b[0;32m~/anaconda3/envs/wave2vec/lib/python3.10/site-packages/huggingface_hub/utils/_http.py:457\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    448\u001b[0m     message \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    449\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Client Error.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    450\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    455\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m https://huggingface.co/docs/huggingface_hub/authentication\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    456\u001b[0m     )\n\u001b[0;32m--> 457\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m _format(RepositoryNotFoundError, message, response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m400\u001b[39m:\n",
            "\u001b[0;31mRepositoryNotFoundError\u001b[0m: 404 Client Error. (Request ID: Root=1-692eac73-33ae5c705b82bd1417cc05eb;e5667ec5-3f97-426e-90a0-03a7f2f2561b)\n\nRepository Not Found for url: https://huggingface.co/facebook/hubert-base-superb/resolve/main/preprocessor_config.json.\nPlease make sure you specified the correct `repo_id` and `repo_type`.\nIf you are trying to access a private or gated repo, make sure you are authenticated. For more details, see https://huggingface.co/docs/huggingface_hub/authentication",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[6], line 7\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# ============================================================================\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# 5. LOAD MODEL\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# ============================================================================\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mLoading HuBERT model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mCONFIG[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhubert_model\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 7\u001b[0m feature_extractor \u001b[38;5;241m=\u001b[39m \u001b[43mWav2Vec2FeatureExtractor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCONFIG\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhubert_model\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m model \u001b[38;5;241m=\u001b[39m HubertForSequenceClassification\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m     10\u001b[0m     CONFIG[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhubert_model\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     11\u001b[0m     num_labels\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(EMOTIONS),\n\u001b[1;32m     12\u001b[0m     id2label\u001b[38;5;241m=\u001b[39mIDX_TO_EMOTION,\n\u001b[1;32m     13\u001b[0m     label2id\u001b[38;5;241m=\u001b[39mEMOTION_TO_IDX\n\u001b[1;32m     14\u001b[0m )\n\u001b[1;32m     16\u001b[0m model \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mto(CONFIG[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
            "File \u001b[0;32m~/anaconda3/envs/wave2vec/lib/python3.10/site-packages/transformers/feature_extraction_utils.py:382\u001b[0m, in \u001b[0;36mFeatureExtractionMixin.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, **kwargs)\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    380\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m token\n\u001b[0;32m--> 382\u001b[0m feature_extractor_dict, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_feature_extractor_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    384\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mfrom_dict(feature_extractor_dict, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[0;32m~/anaconda3/envs/wave2vec/lib/python3.10/site-packages/transformers/feature_extraction_utils.py:508\u001b[0m, in \u001b[0;36mFeatureExtractionMixin.get_feature_extractor_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    505\u001b[0m feature_extractor_file \u001b[38;5;241m=\u001b[39m FEATURE_EXTRACTOR_NAME\n\u001b[1;32m    506\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    507\u001b[0m     \u001b[38;5;66;03m# Load from local folder or from cache or download from model Hub and cache\u001b[39;00m\n\u001b[0;32m--> 508\u001b[0m     resolved_feature_extractor_files \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    509\u001b[0m         resolved_file\n\u001b[1;32m    510\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m filename \u001b[38;5;129;01min\u001b[39;00m [feature_extractor_file, PROCESSOR_NAME]\n\u001b[1;32m    511\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    512\u001b[0m             resolved_file \u001b[38;5;241m:=\u001b[39m cached_file(\n\u001b[1;32m    513\u001b[0m                 pretrained_model_name_or_path,\n\u001b[1;32m    514\u001b[0m                 filename\u001b[38;5;241m=\u001b[39mfilename,\n\u001b[1;32m    515\u001b[0m                 cache_dir\u001b[38;5;241m=\u001b[39mcache_dir,\n\u001b[1;32m    516\u001b[0m                 force_download\u001b[38;5;241m=\u001b[39mforce_download,\n\u001b[1;32m    517\u001b[0m                 proxies\u001b[38;5;241m=\u001b[39mproxies,\n\u001b[1;32m    518\u001b[0m                 resume_download\u001b[38;5;241m=\u001b[39mresume_download,\n\u001b[1;32m    519\u001b[0m                 local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[1;32m    520\u001b[0m                 subfolder\u001b[38;5;241m=\u001b[39msubfolder,\n\u001b[1;32m    521\u001b[0m                 token\u001b[38;5;241m=\u001b[39mtoken,\n\u001b[1;32m    522\u001b[0m                 user_agent\u001b[38;5;241m=\u001b[39muser_agent,\n\u001b[1;32m    523\u001b[0m                 revision\u001b[38;5;241m=\u001b[39mrevision,\n\u001b[1;32m    524\u001b[0m                 _raise_exceptions_for_missing_entries\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    525\u001b[0m             )\n\u001b[1;32m    526\u001b[0m         )\n\u001b[1;32m    527\u001b[0m         \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    528\u001b[0m     ]\n\u001b[1;32m    529\u001b[0m     resolved_feature_extractor_file \u001b[38;5;241m=\u001b[39m resolved_feature_extractor_files[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    530\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[1;32m    531\u001b[0m     \u001b[38;5;66;03m# Raise any environment error raise by `cached_file`. It will have a helpful error message adapted to\u001b[39;00m\n\u001b[1;32m    532\u001b[0m     \u001b[38;5;66;03m# the original exception.\u001b[39;00m\n",
            "File \u001b[0;32m~/anaconda3/envs/wave2vec/lib/python3.10/site-packages/transformers/feature_extraction_utils.py:512\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    505\u001b[0m feature_extractor_file \u001b[38;5;241m=\u001b[39m FEATURE_EXTRACTOR_NAME\n\u001b[1;32m    506\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    507\u001b[0m     \u001b[38;5;66;03m# Load from local folder or from cache or download from model Hub and cache\u001b[39;00m\n\u001b[1;32m    508\u001b[0m     resolved_feature_extractor_files \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    509\u001b[0m         resolved_file\n\u001b[1;32m    510\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m filename \u001b[38;5;129;01min\u001b[39;00m [feature_extractor_file, PROCESSOR_NAME]\n\u001b[1;32m    511\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m--> 512\u001b[0m             resolved_file \u001b[38;5;241m:=\u001b[39m \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    513\u001b[0m \u001b[43m                \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    514\u001b[0m \u001b[43m                \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    515\u001b[0m \u001b[43m                \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    516\u001b[0m \u001b[43m                \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    517\u001b[0m \u001b[43m                \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    518\u001b[0m \u001b[43m                \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    519\u001b[0m \u001b[43m                \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    520\u001b[0m \u001b[43m                \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    521\u001b[0m \u001b[43m                \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m                \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m                \u001b[49m\u001b[43m_raise_exceptions_for_missing_entries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    525\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    526\u001b[0m         )\n\u001b[1;32m    527\u001b[0m         \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    528\u001b[0m     ]\n\u001b[1;32m    529\u001b[0m     resolved_feature_extractor_file \u001b[38;5;241m=\u001b[39m resolved_feature_extractor_files[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    530\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[1;32m    531\u001b[0m     \u001b[38;5;66;03m# Raise any environment error raise by `cached_file`. It will have a helpful error message adapted to\u001b[39;00m\n\u001b[1;32m    532\u001b[0m     \u001b[38;5;66;03m# the original exception.\u001b[39;00m\n",
            "File \u001b[0;32m~/anaconda3/envs/wave2vec/lib/python3.10/site-packages/transformers/utils/hub.py:322\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, **kwargs)\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcached_file\u001b[39m(\n\u001b[1;32m    265\u001b[0m     path_or_repo_id: Union[\u001b[38;5;28mstr\u001b[39m, os\u001b[38;5;241m.\u001b[39mPathLike],\n\u001b[1;32m    266\u001b[0m     filename: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m    267\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    268\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[1;32m    269\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    270\u001b[0m \u001b[38;5;124;03m    Tries to locate a file in a local folder and repo, downloads and cache it if necessary.\u001b[39;00m\n\u001b[1;32m    271\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    320\u001b[0m \u001b[38;5;124;03m    ```\u001b[39;00m\n\u001b[1;32m    321\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 322\u001b[0m     file \u001b[38;5;241m=\u001b[39m \u001b[43mcached_files\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilenames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    323\u001b[0m     file \u001b[38;5;241m=\u001b[39m file[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m file\n\u001b[1;32m    324\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m file\n",
            "File \u001b[0;32m~/anaconda3/envs/wave2vec/lib/python3.10/site-packages/transformers/utils/hub.py:511\u001b[0m, in \u001b[0;36mcached_files\u001b[0;34m(path_or_repo_id, filenames, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    508\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    509\u001b[0m     \u001b[38;5;66;03m# We cannot recover from them\u001b[39;00m\n\u001b[1;32m    510\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, RepositoryNotFoundError) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, GatedRepoError):\n\u001b[0;32m--> 511\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[1;32m    512\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not a local folder and is not a valid model identifier \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    513\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlisted on \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/models\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mIf this is a private repository, make sure to pass a token \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    514\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhaving permission to this repo either by logging in with `hf auth login` or by passing \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    515\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`token=<your_token>`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    516\u001b[0m         ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    517\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, RevisionNotFoundError):\n\u001b[1;32m    518\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[1;32m    519\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrevision\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not a valid git identifier (branch name, tag name or commit id) that exists \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    520\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfor this model name. Check the model page at \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    521\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m for available revisions.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    522\u001b[0m         ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n",
            "\u001b[0;31mOSError\u001b[0m: facebook/hubert-base-superb is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`"
          ]
        }
      ],
      "source": [
        "\n",
        "# ============================================================================\n",
        "# 5. LOAD MODEL\n",
        "# ============================================================================\n",
        "\n",
        "print(f'\\nLoading HuBERT model: {CONFIG[\"hubert_model\"]}')\n",
        "\n",
        "feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(CONFIG['hubert_model'])\n",
        "\n",
        "model = HubertForSequenceClassification.from_pretrained(\n",
        "    CONFIG['hubert_model'],\n",
        "    num_labels=len(EMOTIONS),\n",
        "    id2label=IDX_TO_EMOTION,\n",
        "    label2id=EMOTION_TO_IDX\n",
        ")\n",
        "\n",
        "model = model.to(CONFIG['device'])\n",
        "\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print('✓ Model loaded')\n",
        "print(f'  Total parameters: {total_params:,}')\n",
        "print(f'  Trainable parameters: {trainable_params:,}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cc67d4ca",
      "metadata": {
        "id": "cc67d4ca"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "50f57b20",
      "metadata": {
        "id": "50f57b20"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ============================================================================\n",
        "# 6. CREATE DATA LOADERS\n",
        "# ============================================================================\n",
        "\n",
        "train_dataset = EmotionAudioDataset(train_set, feature_extractor, sr=CONFIG['sr'])\n",
        "val_dataset = EmotionAudioDataset(val_set, feature_extractor, sr=CONFIG['sr'])\n",
        "test_dataset = EmotionAudioDataset(test_set, feature_extractor, sr=CONFIG['sr'])\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=CONFIG['batch_size'],\n",
        "    shuffle=True,\n",
        "    collate_fn=collate_fn,\n",
        "    num_workers=0\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=CONFIG['batch_size'],\n",
        "    shuffle=False,\n",
        "    collate_fn=collate_fn\n",
        ")\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=CONFIG['batch_size'],\n",
        "    shuffle=False,\n",
        "    collate_fn=collate_fn\n",
        ")\n",
        "\n",
        "print('✓ Data loaders created')\n",
        "print(f'  Train batches: {len(train_loader)}')\n",
        "print(f'  Val batches: {len(val_loader)}')\n",
        "print(f'  Test batches: {len(test_loader)}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8cb6fb51",
      "metadata": {
        "id": "8cb6fb51"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ============================================================================\n",
        "# 7. TRAINING SETUP\n",
        "# ============================================================================\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=CONFIG['learning_rate'])\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "total_steps = len(train_loader) * CONFIG['epochs']\n",
        "scheduler = LinearLR(optimizer, start_factor=1.0, end_factor=0.1, total_iters=total_steps)\n",
        "\n",
        "print('✓ Optimizer and scheduler configured')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "25cae038",
      "metadata": {
        "id": "25cae038"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ============================================================================\n",
        "# 8. TRAINING FUNCTIONS\n",
        "# ============================================================================\n",
        "\n",
        "def train_epoch(model, dataloader, optimizer, scheduler, criterion, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    pbar = tqdm(dataloader, desc='Training')\n",
        "    for batch in pbar:\n",
        "        if batch is None:\n",
        "            continue\n",
        "\n",
        "        input_values = batch['input_values'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(input_values, labels=labels)\n",
        "        loss = outputs.loss\n",
        "\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        preds = torch.argmax(outputs.logits, dim=1)\n",
        "        all_preds.extend(preds.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "        pbar.set_postfix({'loss': loss.item()})\n",
        "\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    accuracy = accuracy_score(all_labels, all_preds)\n",
        "\n",
        "    return avg_loss, accuracy\n",
        "\n",
        "def validate(model, dataloader, criterion, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(dataloader, desc='Validating'):\n",
        "            if batch is None:\n",
        "                continue\n",
        "\n",
        "            input_values = batch['input_values'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            outputs = model(input_values, labels=labels)\n",
        "            loss = outputs.loss\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            preds = torch.argmax(outputs.logits, dim=1)\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    accuracy = accuracy_score(all_labels, all_preds)\n",
        "\n",
        "    return avg_loss, accuracy, all_preds, all_labels\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a5cdc1e8",
      "metadata": {
        "id": "a5cdc1e8"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ============================================================================\n",
        "# 9. TRAINING LOOP\n",
        "# ============================================================================\n",
        "\n",
        "history = {\n",
        "    'train_loss': [],\n",
        "    'train_acc': [],\n",
        "    'val_loss': [],\n",
        "    'val_acc': []\n",
        "}\n",
        "\n",
        "best_val_loss = float('inf')\n",
        "patience = 5\n",
        "patience_counter = 0\n",
        "\n",
        "print('\\n' + '='*60)\n",
        "print('TRAINING')\n",
        "print('='*60)\n",
        "\n",
        "for epoch in range(CONFIG['epochs']):\n",
        "    print(f'\\nEpoch [{epoch+1}/{CONFIG[\"epochs\"]}]')\n",
        "\n",
        "    train_loss, train_acc = train_epoch(\n",
        "        model, train_loader, optimizer, scheduler, criterion, CONFIG['device']\n",
        "    )\n",
        "\n",
        "    val_loss, val_acc, _, _ = validate(model, val_loader, criterion, CONFIG['device'])\n",
        "\n",
        "    history['train_loss'].append(train_loss)\n",
        "    history['train_acc'].append(train_acc)\n",
        "    history['val_loss'].append(val_loss)\n",
        "    history['val_acc'].append(val_acc)\n",
        "\n",
        "    print(f'  Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}')\n",
        "    print(f'  Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}')\n",
        "\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        patience_counter = 0\n",
        "        torch.save(model.state_dict(), 'hubert_emotion_model.pt')\n",
        "        print('  ✓ Model saved')\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "        if patience_counter >= patience:\n",
        "            print(f'\\n⏹ Early stopping after {epoch+1} epochs')\n",
        "            break\n",
        "\n",
        "print('\\n' + '='*60)\n",
        "print('✓ Training completed')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "75d6aacc",
      "metadata": {
        "id": "75d6aacc"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ============================================================================\n",
        "# 10. EVALUATION\n",
        "# ============================================================================\n",
        "\n",
        "model.load_state_dict(torch.load('hubert_emotion_model.pt'))\n",
        "\n",
        "test_loss, test_acc, test_preds, test_labels = validate(\n",
        "    model, test_loader, criterion, CONFIG['device']\n",
        ")\n",
        "\n",
        "print('\\n' + '='*60)\n",
        "print('TEST RESULTS')\n",
        "print('='*60)\n",
        "print(f'Test Loss: {test_loss:.4f}')\n",
        "print(f'Test Accuracy: {test_acc:.4f}')\n",
        "print(f'\\nClassification Report:\\n')\n",
        "print(classification_report(test_labels, test_preds, target_names=EMOTIONS))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "946a09c1",
      "metadata": {
        "id": "946a09c1"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ============================================================================\n",
        "# 11. VISUALIZATION\n",
        "# ============================================================================\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "axes[0].plot(history['train_loss'], label='Train Loss', marker='o')\n",
        "axes[0].plot(history['val_loss'], label='Val Loss', marker='s')\n",
        "axes[0].set_xlabel('Epoch')\n",
        "axes[0].set_ylabel('Loss')\n",
        "axes[0].set_title('Training and Validation Loss')\n",
        "axes[0].legend()\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "axes[1].plot(history['train_acc'], label='Train Accuracy', marker='o')\n",
        "axes[1].plot(history['val_acc'], label='Val Accuracy', marker='s')\n",
        "axes[1].set_xlabel('Epoch')\n",
        "axes[1].set_ylabel('Accuracy')\n",
        "axes[1].set_title('Training and Validation Accuracy')\n",
        "axes[1].legend()\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('hubert_training_history.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print('✓ Training history plot saved')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4fec94ac",
      "metadata": {
        "id": "4fec94ac"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ============================================================================\n",
        "# 12. CONFUSION MATRIX\n",
        "# ============================================================================\n",
        "\n",
        "cm = confusion_matrix(test_labels, test_preds)\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=EMOTIONS, yticklabels=EMOTIONS)\n",
        "plt.title('Confusion Matrix - HuBERT Emotion Recognition')\n",
        "plt.ylabel('True Label')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.tight_layout()\n",
        "plt.savefig('hubert_confusion_matrix.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print('✓ Confusion matrix plot saved')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ecb6dbae",
      "metadata": {
        "id": "ecb6dbae"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ============================================================================\n",
        "# 13. PER-EMOTION METRICS\n",
        "# ============================================================================\n",
        "\n",
        "precision, recall, f1, support = precision_recall_fscore_support(\n",
        "    test_labels, test_preds, labels=range(len(EMOTIONS))\n",
        ")\n",
        "\n",
        "metrics_df = pd.DataFrame({\n",
        "    'Emotion': EMOTIONS,\n",
        "    'Precision': precision,\n",
        "    'Recall': recall,\n",
        "    'F1-Score': f1,\n",
        "    'Support': support\n",
        "})\n",
        "\n",
        "print('\\nPer-Emotion Metrics:')\n",
        "print(metrics_df.to_string(index=False))\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(12, 6))\n",
        "metrics_df.set_index('Emotion')[['Precision', 'Recall', 'F1-Score']].plot(kind='bar', ax=ax)\n",
        "plt.title('Per-Emotion Performance Metrics - HuBERT')\n",
        "plt.ylabel('Score')\n",
        "plt.xlabel('Emotion')\n",
        "plt.legend(loc='best')\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.savefig('hubert_per_emotion_metrics.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print('✓ Per-emotion metrics plot saved')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b6e2e06e",
      "metadata": {
        "id": "b6e2e06e"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ============================================================================\n",
        "# 14. INFERENCE\n",
        "# ============================================================================\n",
        "\n",
        "def predict_emotion(audio_path, model, feature_extractor, device, sr=16000, max_length=160000):\n",
        "    \"\"\"Predict emotion for a single audio file.\"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    y, _ = librosa.load(audio_path, sr=sr)\n",
        "\n",
        "    if len(y) < max_length:\n",
        "        y = np.pad(y, (0, max_length - len(y)), mode='constant')\n",
        "    else:\n",
        "        y = y[:max_length]\n",
        "\n",
        "    inputs = feature_extractor(y, sampling_rate=sr, return_tensors='pt')\n",
        "    input_values = inputs['input_values'].to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_values)\n",
        "        logits = outputs.logits\n",
        "        probabilities = torch.nn.functional.softmax(logits, dim=1)\n",
        "        predicted_id = torch.argmax(logits, dim=1).item()\n",
        "        predicted_emotion = IDX_TO_EMOTION[predicted_id]\n",
        "        confidence = probabilities[0, predicted_id].item()\n",
        "\n",
        "    return {\n",
        "        'predicted_emotion': predicted_emotion,\n",
        "        'confidence': confidence,\n",
        "        'probabilities': {\n",
        "            EMOTIONS[i]: probabilities[0, i].item()\n",
        "            for i in range(len(EMOTIONS))\n",
        "        }\n",
        "    }\n",
        "\n",
        "if len(test_set) > 0:\n",
        "    test_audio_path = test_set[0][0]\n",
        "    true_emotion = test_set[0][1]\n",
        "\n",
        "    result = predict_emotion(test_audio_path, model, feature_extractor, CONFIG['device'])\n",
        "\n",
        "    print('\\nExample Prediction:')\n",
        "    print(f'  Audio: {os.path.basename(test_audio_path)}')\n",
        "    print(f'  True Emotion: {true_emotion}')\n",
        "    print(f'  Predicted Emotion: {result[\"predicted_emotion\"]}')\n",
        "    print(f'  Confidence: {result[\"confidence\"]*100:.2f}%')\n",
        "    print(f'\\n  All Probabilities:')\n",
        "    for emotion, prob in result['probabilities'].items():\n",
        "        print(f'    {emotion}: {prob*100:.2f}%')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3c8e6845",
      "metadata": {
        "id": "3c8e6845"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "# ============================================================================\n",
        "# 15. SAVE RESULTS\n",
        "# ============================================================================\n",
        "\n",
        "model.save_pretrained('./hubert_emotion_model')\n",
        "feature_extractor.save_pretrained('./hubert_emotion_model')\n",
        "\n",
        "results = {\n",
        "    'model': 'HuBERT',\n",
        "    'config': CONFIG,\n",
        "    'emotions': EMOTIONS,\n",
        "    'test_accuracy': float(test_acc),\n",
        "    'test_loss': float(test_loss),\n",
        "    'metrics': metrics_df.to_dict(),\n",
        "    'confusion_matrix': cm.tolist()\n",
        "}\n",
        "\n",
        "results['config']['device'] = str(results['config']['device'])\n",
        "\n",
        "with open('hubert_results.json', 'w') as f:\n",
        "    json.dump(results, f, indent=2)\n",
        "\n",
        "print('\\n✓ Model and results saved successfully')\n",
        "print('  - Model: ./hubert_emotion_model/')\n",
        "print('  - Results: hubert_results.json')\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# ============================================================================\n",
        "# 15. SAVE RESULTS  (HuBERT + Time Mask Augmentation)\n",
        "# ============================================================================\n",
        "\n",
        "# Save model and feature extractor for Time Mask experiment\n",
        "model.save_pretrained('./hubert_timemask_emotion_model')\n",
        "feature_extractor.save_pretrained('./hubert_timemask_emotion_model')\n",
        "\n",
        "results = {\n",
        "    'model': 'HuBERT (Time Mask)',\n",
        "    'config': CONFIG,\n",
        "    'emotions': EMOTIONS,\n",
        "    'test_accuracy': float(test_acc),\n",
        "    'test_loss': float(test_loss),\n",
        "    'metrics': metrics_df.to_dict(),\n",
        "    'confusion_matrix': cm.tolist()\n",
        "}\n",
        "\n",
        "# Make device JSON-serializable\n",
        "results['config']['device'] = str(results['config']['device'])\n",
        "\n",
        "# Save results for this specific augmentation\n",
        "with open('hubert_timemask_results.json', 'w') as f:\n",
        "    json.dump(results, f, indent=2)\n",
        "\n",
        "print('\\n✓ Model and results saved successfully (Time Mask)')\n",
        "print('  - Model: ./hubert_timemask_emotion_model/')\n",
        "print('  - Results: hubert_timemask_results.json')\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a485dd3a",
      "metadata": {
        "id": "a485dd3a"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "# ============================================================================\n",
        "# 16. SUMMARY\n",
        "# ============================================================================\n",
        "\n",
        "print('\\n' + '='*60)\n",
        "print('EMOTION RECOGNITION WITH HUBERT - SUMMARY')\n",
        "print('='*60)\n",
        "print(f'\\nDataset:')\n",
        "print(f'  Total samples: {len(file_paths)}')\n",
        "print(f'  Train: {len(train_set)}, Val: {len(val_set)}, Test: {len(test_set)}')\n",
        "print(f'\\nModel:')\n",
        "print(f'  Architecture: {CONFIG[\"hubert_model\"]}')\n",
        "print(f'  Total parameters: {total_params:,}')\n",
        "print(f'  Trainable parameters: {trainable_params:,}')\n",
        "print(f'\\nResults:')\n",
        "print(f'  Test Accuracy: {test_acc:.4f}')\n",
        "print(f'  Test Loss: {test_loss:.4f}')\n",
        "print(f'  Emotions: {EMOTIONS}')\n",
        "print('='*60)\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# ============================================================================\n",
        "# 16. SUMMARY  (HuBERT + Time Mask Augmentation)\n",
        "# ============================================================================\n",
        "\n",
        "AUG_NAME = \"Time Mask\"   # <-- Change this for other augmentations\n",
        "\n",
        "print('\\n' + '='*60)\n",
        "print(f'EMOTION RECOGNITION WITH HUBERT ({AUG_NAME}) - SUMMARY')\n",
        "print('='*60)\n",
        "\n",
        "print(f'\\nDataset:')\n",
        "print(f'  Total samples: {len(file_paths)}')\n",
        "print(f'  Train: {len(train_set)}, Val: {len(val_set)}, Test: {len(test_set)}')\n",
        "\n",
        "print(f'\\nAugmentation Used:')\n",
        "print(f'  Applied Augmentation: {AUG_NAME}')\n",
        "\n",
        "print(f'\\nModel:')\n",
        "print(f'  Architecture: {CONFIG[\"hubert_model\"]}')\n",
        "print(f'  Total parameters: {total_params:,}')\n",
        "print(f'  Trainable parameters: {trainable_params:,}')\n",
        "\n",
        "print(f'\\nResults:')\n",
        "print(f'  Test Accuracy: {test_acc:.4f}')\n",
        "print(f'  Test Loss: {test_loss:.4f}')\n",
        "print(f'  Emotions: {EMOTIONS}')\n",
        "\n",
        "print('='*60)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "wave2vec",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}